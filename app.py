import streamlit as st
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import time
import os
import gdown # C·∫ßn th∆∞ vi·ªán n√†y ƒë·ªÉ t·∫£i file t·ª´ Google Drive

# T√äN FILE V√Ä FILE ID
DATA_FILE = 'anime_dataset_small_nomic.parquet'
# !! QUAN TR·ªåNG: B·∫†N PH·∫¢I THAY TH·∫æ ID N√ÄY B·∫∞NG ID FILE C·ª¶A B·∫†N T·ª™ GOOGLE DRIVE !!
DATA_FILE_ID = 'https://drive.google.com/file/d/16bdNhA2DCgRevE3ZtaQIIym_lSRYVqQO/view?usp=sharing' 
MODEL_NAME = 'nomic-ai/nomic-embed-text-v1.5'

# --- H√ÄM T·∫¢I FILE N·∫∂NG (S·ª¨ D·ª§NG CACHE) ---
@st.cache_resource
def load_data_and_initialize_rag():
    st.info(f"B·∫Øt ƒë·∫ßu: T·∫£i v√† Kh·ªüi t·∫°o H·ªá th·ªëng RAG...")
    
    # 1. T·∫¢I FILE D·ªÆ LI·ªÜU T·ª™ GOOGLE DRIVE N·∫æU CH∆ØA T·ªíN T·∫†I
    if not os.path.exists(DATA_FILE):
        if DATA_FILE_ID == 'https://drive.google.com/file/d/16bdNhA2DCgRevE3ZtaQIIym_lSRYVqQO/view?usp=sharing':
            st.error("L·ªñI TRI·ªÇN KHAI: B·∫°n ch∆∞a thay th·∫ø DATA_FILE_ID b·∫±ng ID file Google Drive c·ªßa m√¨nh.")
            return None, None, None
            
        st.info(f"ƒêang t·∫£i file data l·ªõn t·ª´ Google Drive (ID: {DATA_FILE_ID})...")
        try:
            # Gdown s·∫Ω t·∫£i file v√† l∆∞u v·ªõi t√™n DATA_FILE
            gdown.download(id=DATA_FILE_ID, output=DATA_FILE, quiet=False, fuzzy=True)
            st.success("T·∫£i file data th√†nh c√¥ng!")
        except Exception as e:
            st.error(f"L·ªñI T·∫¢I FILE: Kh√¥ng th·ªÉ t·∫£i file t·ª´ Google Drive. ƒê·∫£m b·∫£o ID v√† quy·ªÅn chia s·∫ª c√¥ng khai l√† ƒë√∫ng. L·ªói: {e}")
            return None, None, None
    else:
        st.info("File data ƒë√£ t·ªìn t·∫°i, ti·∫øn h√†nh ƒë·ªçc file.")
    
    # 2. ƒê·ªåC D·ªÆ LI·ªÜU
    try:
        df = pd.read_parquet(DATA_FILE)
    except Exception as e:
        st.error(f"L·ªñI ƒê·ªåC FILE: Kh√¥ng th·ªÉ ƒë·ªçc file Parquet. L·ªói: {e}")
        return None, None, None

    # 3. T·∫†O TR∆Ø·ªúNG CONTEXT RAG
    st.info("B∆∞·ªõc 1: T·∫°o tr∆∞·ªùng 'rag_context'...")
    try:
        df['rag_context'] = (
            "Title: " + df['Main Title'].fillna('Unknown Title') + " | " +
            "Studio: " + df['Animation Work'].fillna('Unknown Studio') + " | " +
            "Tags: " + df['Tags'].fillna('No tags') + " | " + 
            "Synopsis: " + df['Synopsis'].fillna('No synopsis provided')
        )
    except KeyError as e:
        st.error(f"L·ªñ·ªñI KEY: C·ªôt {e} kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ch√≠nh t·∫£ t√™n c·ªôt.")
        return None, None, None

    # 4. T·∫£i M√¥ h√¨nh Embedding 
    st.info(f"B∆∞·ªõc 2: T·∫£i m√¥ h√¨nh Embedding: {MODEL_NAME}...")
    try:
        model = SentenceTransformer(MODEL_NAME, trust_remote_code=True)
    except Exception as e:
        st.error(f"L·ªñI: Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh embedding. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet. L·ªói chi ti·∫øt: {e}")
        return None, None, None
    
    # 5. T·∫°o Embeddings v√† Index FAISS
    st.info("B∆∞·ªõc 3: T·∫°o Embeddings v√† Index FAISS...")
    embedding_texts = df['rag_context'].tolist()
    
    embeddings = model.encode(embedding_texts, show_progress_bar=False)
    
    # T·∫°o Index FAISS
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings).astype('float32'))
    
    st.success(f"Kh·ªüi t·∫°o RAG th√†nh c√¥ng! T·ªïng s·ªë entries: {len(df)}")
    return df, model, index

# --- 2. H√†m T√¨m ki·∫øm Ng·ªØ nghƒ©a ---
def semantic_search(query: str, df: pd.DataFrame, model: SentenceTransformer, index: faiss.Index, k: int = 5):
    """Th·ª±c hi·ªán t√¨m ki·∫øm vector v√† tr·∫£ v·ªÅ c√°c anime ph√π h·ª£p nh·∫•t."""
    
    # 2.1. Embed Query
    query_embedding = model.encode([query]) 
    
    # 2.2. T√¨m ki·∫øm trong Index FAISS
    distances, indices = index.search(np.array(query_embedding).astype('float32'), k)
    
    # 2.3. L·∫•y k·∫øt qu·∫£ t·ª´ DataFrame g·ªëc
    results = df.iloc[indices[0]].copy()
    
    # Th√™m kho·∫£ng c√°ch L2 v√†o k·∫øt qu·∫£
    results['Distance'] = distances[0]
    
    return results.sort_values(by='Distance', ascending=True)

# --- 3. Giao di·ªán Streamlit ---

st.title("ü§ñ Anime Recommender RAG (Public)") # ƒê·ªïi t√™n cho b·∫£n Public

# Kh·ªüi t·∫°o h·ªá th·ªëng
df, model, index = load_data_and_initialize_rag()

if df is not None:
    st.subheader("Ho√†n t·∫•t Kh·ªüi t·∫°o. B√¢y gi·ªù b·∫°n c√≥ th·ªÉ t√¨m ki·∫øm.")
    
    # Thanh t√¨m ki·∫øm
    user_query = st.text_input(
        "Nh·∫≠p truy v·∫•n b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n:",
        "Dark fantasy anime with tragic character arcs and moral ambiguity"
    )
    
    k_recommendations = st.slider("S·ªë l∆∞·ª£ng ƒë·ªÅ xu·∫•t:", 1, 10, 5)

    if user_query:
        start_time = time.time()
        
        # Th·ª±c hi·ªán t√¨m ki·∫øm
        with st.spinner("ƒêang t√¨m ki·∫øm ng·ªØ nghƒ©a..."):
            recommendations = semantic_search(user_query, df, model, index, k_recommendations)
        
        end_time = time.time()
        
        st.subheader(f"Top {k_recommendations} ƒê·ªÅ xu·∫•t Anime:")
        st.write(f"*T√¨m ki·∫øm ho√†n t·∫•t trong {end_time - start_time:.4f} gi√¢y.*")
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        for i, row in recommendations.iterrows():
            st.markdown("---")
            main_title = row.get('Main Title', 'N/A')
            official_en = row.get('Official Title (en)', 'N/A')
            max_rating = row.get('Max Rating', 0.0)
            filter_year = int(row.get('filter_year', 0))
            animation_work = row.get('Animation Work', 'N/A')
            synopsis = row.get('Synopsis', 'Kh√¥ng c√≥ t√≥m t·∫Øt')
            tags_content = row.get('Tags', 'Kh√¥ng c√≥ th·∫ª')
            
            st.markdown(f"**{main_title}** (Official EN: {official_en})")
            st.markdown(f"**Rating:** {max_rating:.2f} | **NƒÉm:** {filter_year} | **Studio:** {animation_work}")
            st.markdown(f"**Tags:** *{tags_content}*")
            st.markdown(f"**Synopsis:** {synopsis}")
            st.caption(f"ƒê·ªô g·∫ßn (L2 Distance): {row['Distance']:.4f}")