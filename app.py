import streamlit as st
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import time
import os
import gdown 

# C·∫§U H√åNH DATA V√Ä MODEL (ID GOOGLE DRIVE C·ª¶A B·∫†N ƒê√É ƒê∆Ø·ª¢C D√ÅN V√ÄO ƒê√ÇY)
DATA_FILE = 'anime_dataset_small_nomic.parquet'
DATA_FILE_ID = '16bdNhA2DCgRevE3ZtaQIIym_lSRYVqQO' 
MODEL_NAME = 'nomic-ai/nomic-embed-text-v1.5'

# --- 1. T·∫£i D·ªØ li·ªáu, T·∫°o Index v√† Model (Ch·ªâ ch·∫°y 1 l·∫ßn) ---
@st.cache_resource
def load_data_and_initialize_rag():
    # 1. T·∫¢I FILE D·ªÆ LI·ªÜU T·ª™ GOOGLE DRIVE N·∫æU CH∆ØA T·ªíN T·∫†I
    if not os.path.exists(DATA_FILE):
        try:
            # Gdown s·∫Ω t·∫£i file v√† l∆∞u v·ªõi t√™n DATA_FILE
            gdown.download(id=DATA_FILE_ID, output=DATA_FILE, quiet=True, fuzzy=True)
        except Exception as e:
            st.error(f"L·ªñI T·∫¢I DATA: Kh√¥ng th·ªÉ t·∫£i file t·ª´ Google Drive. Vui l√≤ng ki·ªÉm tra ID v√† quy·ªÅn chia s·∫ª. L·ªói: {e}")
            return None, None, None
    
    # 2. ƒê·ªåC D·ªÆ LI·ªÜU
    try:
        df = pd.read_parquet(DATA_FILE)
    except Exception as e:
        st.error(f"L·ªñI ƒê·ªåC FILE: Kh√¥ng th·ªÉ ƒë·ªçc file Parquet. L·ªói: {e}")
        return None, None, None

    # 3. T·∫†O TR∆Ø·ªúNG CONTEXT RAG
    try:
        df['rag_context'] = (
            "Title: " + df['Main Title'].fillna('Unknown Title') + " | " +
            "Studio: " + df['Animation Work'].fillna('Unknown Studio') + " | " +
            "Tags: " + df['Tags'].fillna('No tags') + " | " + 
            "Synopsis: " + df['Synopsis'].fillna('No synopsis provided')
        )
    except KeyError as e:
        st.error(f"L·ªñ·ªñI KEY: C·ªôt {e} kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ch√≠nh t·∫£ t√™n c·ªôt.")
        return None, None, None

    # 4. T·∫£i M√¥ h√¨nh Embedding 
    try:
        model = SentenceTransformer(MODEL_NAME, trust_remote_code=True)
    except Exception as e:
        st.error(f"L·ªñI: Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh embedding. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet. L·ªói chi ti·∫øt: {e}")
        return None, None, None
    
    # 5. T·∫°o Embeddings v√† Index FAISS
    embedding_texts = df['rag_context'].tolist()
    embeddings = model.encode(embedding_texts, show_progress_bar=False)
    
    # T·∫°o Index FAISS
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings).astype('float32'))
    
    return df, model, index

# --- 2. H√†m T√¨m ki·∫øm Ng·ªØ nghƒ©a ---
def semantic_search(query: str, df: pd.DataFrame, model: SentenceTransformer, index: faiss.Index, k: int = 5):
    """Th·ª±c hi·ªán t√¨m ki·∫øm vector v√† tr·∫£ v·ªÅ c√°c anime ph√π h·ª£p nh·∫•t."""
    
    query_embedding = model.encode([query]) 
    distances, indices = index.search(np.array(query_embedding).astype('float32'), k)
    
    results = df.iloc[indices[0]].copy()
    results['Distance'] = distances[0]
    
    return results.sort_values(by='Distance', ascending=True)

# --- 3. Giao di·ªán Streamlit ---

# C·∫•u h√¨nh trang (ch·∫ø ƒë·ªô Wide)
st.set_page_config(
    page_title="AiMi Anime Recommender",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Ti√™u ƒë·ªÅ ch√≠nh
st.markdown("<h1 style='text-align: center; color: #FF69B4;'>üíñ AiMi Anime Recommender ü§ñ</h1>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align: center; color: #808080;'>T√¨m ki·∫øm Anime b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n s·ª≠ d·ª•ng Vector AI</h4>", unsafe_allow_html=True)


# S·ª≠ d·ª•ng st.spinner ƒë·ªÉ ·∫©n c√°c b∆∞·ªõc k·ªπ thu·∫≠t
with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng ƒê·ªÅ xu·∫•t AI... (Ch·ªâ l·∫ßn ƒë·∫ßu ti√™n t·∫£i s·∫Ω l√¢u)"):
    df, model, index = load_data_and_initialize_rag()

if df is not None:
    # Thay st.success b·∫±ng st.toast ƒë·ªÉ th√¥ng b√°o g·ªçn g√†ng h∆°n
    st.toast("H·ªá th·ªëng AI ƒë√£ s·∫µn s√†ng!")
    st.markdown("---")
    
    # CONTAINER CHO THANH T√åM KI·∫æM V√Ä SLIDER
    search_container = st.container()
    with search_container:
        col1, col2 = st.columns([4, 1])
        
        with col1:
            user_query = st.text_input(
                "üí¨ Nh·∫≠p m√¥ t·∫£ Anime b·∫°n mu·ªën t√¨m:",
                "Dark fantasy anime with tragic character arcs and moral ambiguity",
                placeholder="V√≠ d·ª•: Slice of life comedy set in high school with healing atmosphere"
            )
        
        with col2:
            # Thay ƒë·ªïi nh√£n slider
            k_recommendations = st.slider("S·ªë l∆∞·ª£ng ƒë·ªÅ xu·∫•t:", 1, 10, 5, help="Ch·ªçn s·ªë l∆∞·ª£ng anime b·∫°n mu·ªën ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t.")

    # KH·ªûI CH·∫†Y T√åM KI·∫æM
    if user_query:
        start_time = time.time()
        
        # Th·ª±c hi·ªán t√¨m ki·∫øm
        with st.spinner(f"üîç ƒêang t√¨m ki·∫øm ng·ªØ nghƒ©a cho '{user_query}'..."):
            recommendations = semantic_search(user_query, df, model, index, k_recommendations)
        
        end_time = time.time()
        
        # Ti√™u ƒë·ªÅ k·∫øt qu·∫£
        st.markdown(f"## Top {k_recommendations} ƒê·ªÅ xu·∫•t Ph√π h·ª£p:")
        st.caption(f"üîé Ho√†n t·∫•t t√¨m ki·∫øm trong {end_time - start_time:.4f} gi√¢y.")
        
        # HI·ªÇN TH·ªä K·∫æT QU·∫¢ D∆Ø·ªöI D·∫†NG CARD
        for i, row in recommendations.iterrows():
            with st.container(border=True):
                main_title = row.get('Main Title', 'N/A')
                official_en = row.get('Official Title (en)', 'N/A')
                max_rating = row.get('Max Rating', 0.0)
                filter_year = int(row.get('filter_year', 0))
                animation_work = row.get('Animation Work', 'N/A')
                synopsis = row.get('Synopsis', 'Kh√¥ng c√≥ t√≥m t·∫Øt')
                tags_content = row.get('Tags', 'Kh√¥ng c√≥ th·∫ª')
                
                # T√≠nh to√°n l·∫°i Similarity Score (Chuy·ªÉn L2 Distance v·ªÅ ƒëi·ªÉm t·ª´ 0-1)
                # Max L2 Distance c√≥ th·ªÉ kho·∫£ng 2.0. Chu·∫©n h√≥a v·ªÅ 0-100%
                # D√πng np.clip ƒë·ªÉ tr√°nh gi√° tr·ªã √¢m/l·ªõn v√¥ l√Ω
                normalized_distance = np.clip(row['Distance'], 0, 1.5) 
                similarity_percentage = np.clip(100 - (normalized_distance * 100 / 1.5), 0, 100) # Chu·∫©n h√≥a d·ª±a tr√™n max distance 1.5
                
                col_info, col_rating = st.columns([3, 1])
                
                with col_info:
                    # Ti√™u ƒë·ªÅ ch√≠nh + NƒÉm s·∫£n xu·∫•t
                    st.markdown(f"### üèÜ {main_title} *({official_en})*")
                    st.markdown(f"**üé¨ Studio:** {animation_work} | **üìÖ NƒÉm:** {filter_year}")
                    
                    # R√∫t g·ªçn Tags n·∫øu qu√° d√†i
                    if len(tags_content) > 150:
                        display_tags = tags_content[:150] + "..."
                    else:
                        display_tags = tags_content
                    
                    st.markdown(f"**üè∑Ô∏è Th·ªÉ lo·∫°i ch√≠nh:** *{display_tags}*")
                    st.markdown(f"**üìñ T√≥m t·∫Øt:** {synopsis}")
                
                with col_rating:
                    # Hi·ªÉn th·ªã Rating v√† ƒê·ªô T∆∞∆°ng ƒë·ªìng b·∫±ng st.metric
                    st.metric(label="‚≠ê ƒê√°nh gi√° (10)", value=f"{max_rating:.2f}")
                    # Hi·ªÉn th·ªã ƒë·ªô t∆∞∆°ng ƒë·ªìng d∆∞·ªõi d·∫°ng %
                    st.metric(label="üéØ ƒê·ªô t∆∞∆°ng ƒë·ªìng", value=f"{similarity_percentage:.1f}%")
